import { GoogleGenAI } from "@google/genai";
import { PERSONAS, PersonaId } from "./personas";
import { getActiveModel } from "./voice-training-service";
import { verifyConstraints } from "./constraint-service";
import { optimizeContent } from "./refinement-service";

/**
 * GENERATED ASSETS SCHEMA
 * -----------------------
 * Represents the structured specific output generated by the AI.
 */
export interface GeneratedAssets {
  /** The main LinkedIn text post content (filtered & formatted). */
  textPost: string;
  /** Detailed prompt for image generation (Visualize Value style). */
  imagePrompt: string;
  /** 60-second video script for vertical video content. */
  videoScript: string;
  /** X (Twitter) Thread as an array of tweets. */
  xThread?: string[];
  /** Substack/Medium Long-form Essay. */
  substackEssay?: string;
  /** Voice note script optimized for audio delivery. */
  audioScript?: string;
  /** Optional URL of the generated image (if image generation was successful). */
  imageUrl?: string;
  /** High-CTR image prompt for YouTube/Blog thumbnails. */
  thumbnailPrompt?: string;
}

/**
 * CORE GENERATION FUNCTION
 * ------------------------
 * Orchestrates the text generation process using Google Gemini.
 * 
 * Flow:
 * 1. Selects the System Prompt based on `personaId`.
 * 2. Configures the Gemini Model (currently using `models/gemini-flash-latest`).
 * 3. Constructs the User Prompt with specific instructions.
 * 4. Calls the LLM and parses the JSON response.
 * 5. Validates the output against constraints (Length, Formatting).
 * 6. Retries automatically if validation fails.
 * 
 * @param input The raw user input (Topic or URL).
 * @param apiKey Google Gemini API Key.
 * @param personaId The selected persona (default: 'cso').
 */
export async function generateContent(
  input: string,
  apiKey: string,
  personaId: PersonaId = "cso",
  signals:any[] = [] 
): Promise<GeneratedAssets> {
  // MOCK MODE for Testing/Demo if key is 'demo'
  if (apiKey.toLowerCase().trim() === "demo") {
    await new Promise(r => setTimeout(r, 2000));
    const personaName = PERSONAS[personaId]?.name || "Unknown";
    return {
      textPost: `[MOCK ${personaName.toUpperCase()} OUTPUT]\n\nSuccess is about avoiding stupidity.\n\nMost people think they need to be smart.\n\nThey are wrong.\n\nYou just need to not be stupid.\n\n- Avoid ruin.\n- Survive long enough.\n- Let compounding work.\n\n(Generated by ${personaName})`,
      xThread: [
        "1/ Success isn't about being smart.",
        "2/ It's about avoiding stupidity.",
        "3/ Most 'geniuses' blow up. The survivor wins.",
        "4/ Avoid ruin, let compounding do the work."
      ],
      substackEssay: "# The Stupidity Paradox\n\nIn this week's exploration, we dive deep into why inverse competence is the secret to 2028 baseline wealth...",
      imagePrompt: "Minimalist vector line art. White on black. High contrast. A maze with one clear exit path.",
      videoScript: "HOOK: Stop trying to be smart. It's killing your gains.\n\nCUT TO: Black screen, white text 'AVOID STUPIDITY'.\n\nVO: In a world of geniuses, the survivor wins."
    };
  }

  // MODEL PRIORITY: Try models/gemini-flash-latest first, fallback to models/gemini-2.5-flash-preview-09-2025 on rate limit
  const PRIMARY_MODEL = "models/gemini-flash-latest";
  const FALLBACK_MODEL = "models/gemini-1.5-flash";
  
  let modelName = PRIMARY_MODEL;
  const persona = PERSONAS[personaId];
  let systemInstruction = (persona?.basePrompt || "") + "\n" + (persona?.jsonSchema || "");

  // Check if using custom voice with fine-tuned model
  if (personaId === "custom") {
    try {
      const activeModel = await getActiveModel();
      
      if (activeModel && activeModel.geminiModelId) {
        console.log(`Using custom voice model: ${activeModel.geminiModelId}`);
        // For tuned models, the model name is the tuned model resource name
        modelName = activeModel.geminiModelId;
        // Tuned models often have the system prompt baked in or we can provide one
        // Usually good to provide the persona context still
        systemInstruction = "You are a LinkedIn creator with a unique voice. Maintain this voice strictly.";
      } else {
        console.warn("Custom persona selected but no trained model found, falling back to base Gemini");
      }
    } catch (e) {
      console.warn("Failed to load custom voice model, falling back to Gemini:", e);
    }
  }

  let signalContext = "";
  if (signals.length > 0) {
    signalContext = `
    REAL-TIME SIGNALS (NEWSJACKING CONTEXT):
    The following events just happened. You MUST incorporate at least one data point to prove timeliness.
    ${signals.map(s => `- [${s.type.toUpperCase()}] ${s.value} (${s.source})`).join("\n")}
    `;
  }

  const prompt = `
    Analyze this input and generate a 5-pillar content strategy for 2028:
    1. LinkedIn Text Post (following the rules).
    2. X Thread (JSON array of 5-10 tweets, punchy, high curiosity gaps, max 280 chars each).
    3. Substack Essay (A long-form, highly intellectual exploration, structured with headers, 1000+ words).
    4. "Visualize Value" Image Prompt (Minimalist vector line art, white on black, geometric representation of the concept).
    5. YouTube Thumbnail Prompt (High-CTR, "MrBeast style" or "Figma minimalist", focusing on facial expression and bold text).
    6. 60s Video Script (Cinematic, viral hook, regular retention cuts).

    INPUT:
    "${input}"
    ${signalContext}

    Ensure the response is valid JSON.
  `;

  // Helper function to attempt generation with a specific model
  async function attemptGeneration(useModel: string): Promise<GeneratedAssets | null> {
    const genAI = new GoogleGenAI({ apiKey });
    
    // Config object for the new SDK
    const generationConfig = {
        responseMimeType: "application/json",
        systemInstruction: systemInstruction
    };

    let attempts = 0;
    const MAX_ATTEMPTS = 2;
    let currentPrompt = prompt;

    while (attempts <= MAX_ATTEMPTS) {
      attempts++;
      try {
        console.log(`[AI Service] Generation Attempt ${attempts} with ${useModel}...`);
        
        // New SDK Method Signature
        const result = await genAI.models.generateContent({
            model: useModel,
            contents: currentPrompt, // Simple string input supported
            config: generationConfig
        });

        // result.text is a getter/property
        const content = result.text;
        if (!content) throw new Error("No content generated");
        
        const parsed = JSON.parse(content);
        let textPost = parsed.textPost || parsed.text_post || "";

        // --- PHASE 3: VIRAL OPTIMIZATION ---
        console.log("ðŸ“ Running Adversarial Refinement Loop...");
        textPost = await optimizeContent(textPost, apiKey);
        
        // VERIFY CONSTRAINTS
        const validation = verifyConstraints(textPost);

        if (validation.valid) {
          console.log(`[AI Service] Constraint Check Passed âœ… (Model: ${useModel})`);
          return {
            textPost,
            imagePrompt: parsed.imagePrompt || parsed.image_prompt || "",
            thumbnailPrompt: parsed.thumbnailPrompt || parsed.thumbnail_prompt || "High contrast close-up with bold text overlay.",
            videoScript: parsed.videoScript || parsed.video_script || "",
            xThread: parsed.xThread || parsed.x_thread || [],
            substackEssay: parsed.substackEssay || parsed.substack_essay || ""
          };
        }

        console.warn(`[AI Service] Constraint Failed âŒ: ${validation.reason}`);
        
        if (attempts <= MAX_ATTEMPTS) {
          currentPrompt += `\n\nCRITICAL SYSTEM ALERT: The previous output failed verification. \nREASON: ${validation.reason}\n\nFIX: Rewrite the hook to be shorter. Strict limit: 210 chars.`;
        } else {
          console.warn("[AI Service] Max attempts reached. Returning best effort.");
          return {
            textPost: `[CONSTRAINT FAILED] ${textPost}`,
            imagePrompt: parsed.imagePrompt || parsed.image_prompt || "",
            videoScript: parsed.videoScript || parsed.video_script || "",
          };
        }
      } catch (e: unknown) {
        const errorMessage = e instanceof Error ? e.message : "Unknown error";
        console.error(`[AI Service] Attempt ${attempts} Error with ${useModel}:`, e);
        
        // RATE LIMIT: Return null to trigger fallback to different model
        if (errorMessage.includes("429") || errorMessage.includes("Quota") || errorMessage.includes("quota")) {
          console.warn(`[AI Service] Rate limit hit on ${useModel}. Will try fallback model.`);
          return null; // Signal to try fallback
        }
        
        // NETWORK/SERVER ERRORS: Return backup content
        if (errorMessage.includes("503") || errorMessage.includes("500") || errorMessage.includes("fetch failed")) {
          console.warn("[AI Service] Network/Server Error. Switching to Backup Content.");
          return {
            textPost: `[BACKUP MODE: API BUSY]\n\nConsistency beats intensity.\n\nEveryone starts strong.\nFew finish.\n\nThe secret isn't a hack.\nIt's showing up when you don't want to.\n\nDon't break the chain.`,
            imagePrompt: "Minimalist vector lines showing a consistent upward trend vs erratic spikes. White on black.",
            videoScript: "HOOK: Why do 99% of people fail?\n\nCUT TO: Graph showing jagged spikes then a crash.\n\nVO: They rely on motivation.\n\nCUT TO: Straight rising line.\n\nVO: Winners rely on discipline.",
          };
        }

        if (attempts > MAX_ATTEMPTS) throw e;
      }
    }
    return null;
  }

  // TRY PRIMARY MODEL FIRST
  console.log(`[AI Service] Trying primary model: ${modelName}`);
  let result = await attemptGeneration(modelName);
  
  // IF RATE LIMITED, TRY FALLBACK MODEL
  if (result === null && modelName === PRIMARY_MODEL) {
    console.log(`[AI Service] Falling back to: ${FALLBACK_MODEL}`);
    result = await attemptGeneration(FALLBACK_MODEL);
  }
  
  if (result) {
    return result;
  }

  throw new Error("AI Generation failed after trying all models.");
}

/**
 * SIDE ASSET GENERATION
 * ---------------------
 * Generates secondary assets (Image Prompt, Video Script) based on the already generated text.
 * Used for the streaming workflow where we first stream text, then fill in the rest.
 */
export async function generateSideAssetsFromText(
  textPost: string,
  apiKey: string,
  personaId: PersonaId = "cso"
): Promise<Pick<GeneratedAssets, "imagePrompt" | "videoScript">> {
  const genAI = new GoogleGenAI({ apiKey });
  
  const prompt = `
    Based on this LinkedIn post text, generate exactly two assets in a JSON object:
    1. "imagePrompt": A "Visualize Value" style Image Prompt (Minimalist, white on black, geometric representation of the concept).
    2. "videoScript": A 60s Video Script as a string.

    POST TEXT:
    "${textPost}"

    JSON SCHEMA:
    {
      "imagePrompt": "string",
      "videoScript": "string"
    }
  `;

  try {
    const result = await genAI.models.generateContent({
        model: "models/gemini-flash-latest",
        contents: prompt,
        config: { responseMimeType: "application/json" }
    });
    const content = result.text;
    if (!content) throw new Error("No content generated");
    const parsed = JSON.parse(content);

    // Extraction with fallbacks for common naming variants from different models
    const imagePrompt = parsed.imagePrompt || 
                       parsed.image_prompt || 
                       parsed.visualize_value_image_prompt || 
                       "Minimalist geometric abstraction of operational velocity.";
                       
    let videoScript = parsed.videoScript || 
                      parsed.video_script || 
                      parsed.video_script_60s || 
                      "Script generation failed.";
    
    // Safety: If videoScript is an object (common with complex schemas), format it as readable string
    if (typeof videoScript === 'object') {
        videoScript = JSON.stringify(videoScript, null, 2)
            .replace(/[{}\[\]",]/g, '')  // Remove JSON syntax characters
            .split('\n')
            .filter(line => line.trim())  // Remove empty lines
            .map(line => line.trim())
            .join('\n');
    }

    return {
      imagePrompt: String(imagePrompt),
      videoScript: String(videoScript),
    };
  } catch (e) {
    console.error("Side asset generation failed:", e);
    return {
      imagePrompt: "Error generating image prompt.",
      videoScript: "Error generating video script."
    };
  }
}

/**
 * GENERATE COMMENT
 * ----------------
 * Generates a reply/comment to a specific post based on a selected tone.
 * Now takes personaId to maintain consistent voice across StrategyOS.
 */
export async function generateComment(
  postContent: string,
  tone: string,
  apiKey: string,
  personaId: PersonaId = "cso"
): Promise<string> {
    const genAI = new GoogleGenAI({ apiKey });
    const modelName = "models/gemini-2.5-flash-preview-09-2025";

    const persona = PERSONAS[personaId];
    const personaContext = persona?.basePrompt || "";

    const prompt = `
        ACT AS: ${persona?.name || "Strategist"}
        CONTEXT: ${personaContext}
        
        TASK:
        You are NOT creating a new post. You are RESPONDING to someone else's post.
        Your goal is to be the "Smartest Person in the Comments" while maintaining the StrategyOS voice.
        
        POST CONTENT TO REPLY TO:
        """
        ${postContent}
        """
        
        COMMENT TONE/GOAL: ${tone}
        
        GUIDELINES FOR A REPLIER:
        1. ACKNOWLEDGE: Briefly acknowledge a specific point from the post (don't just say "Great post").
        2. ADD VALUE: Provide a strategic pivot, a contrarian angle, or a clarifying framework that the author missed.
        3. INTERACT: Treat the author as a peer. Challenge them or support them with logic.
        4. BREVITY: Keep it under 50 words. No "I hope this helps" or fluff.
        5. NO ROBOT SPEAK: Follow the "Anti-Robot Filter" (No 'Delve', 'Unleash', 'Game-changer').
        
        The comment should feel like it's part of a high-level executive conversation.
        
        COMMENT:
    `;

    try {
        const result = await genAI.models.generateContent({
            model: modelName,
            contents: prompt
        });
        return result.text || "Error: No response";
    } catch (e) {
        console.error("Comment generation failed:", e);
        return "Error generating comment. Please try again.";
    }
}
